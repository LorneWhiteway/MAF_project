% http://www.ctan.org/tex-archive/macros/latex/contrib/beamer/examples
% http://latex.artikel-namsu.de/english/beamer-examples.html

%\documentclass{beamer}
\documentclass[usenames,dvipsnames]{beamer}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{fancybox, graphicx}
\usepackage{listings}
\usepackage{tikz} % Diagrams
\usetikzlibrary{positioning}
\usepackage{color}
\usepackage{xcolor}
\usepackage{textcomp} % See https://tex.stackexchange.com/questions/145416/how-to-have-straight-single-quotes-in-lstlistings
%\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures. From https://tex.stackexchange.com/questions/238636
\usepackage[absolute,overlay]{textpos}




\usetheme{boxes}
\usecolortheme{beaver}


\title{MAF: \\ Masked Autoregressive Flow for Density Estimation}
\author{Lorne Whiteway \\ lorne.whiteway@star.ucl.ac.uk}
\institute{Astrophysics Group \\ Department of Physics and Astronomy \\ University College London}
\date{27 April 2020 \\ Find the presentation at \alert{\url{https://tinyurl.com/???}}}

\begin{document}

\frame{\titlepage}

\begin{frame}{Context}
  \begin{block}{}
    \begin{itemize}
      \item{\textit{Supervised machine learning} i.e. selecting from a highly-parameterised family of non-linear functions to fit some training data.}
      \item{Fitting is done to optimise a utility function that \textcolor{red}{rewards a close match to the training data} and \textcolor{blue}{penalises complexity i.e. enforces \textit{regularization}}.}
      \item{\textcolor{blue}{Regularisation is also provided by any lack-of-flexibility in the fitting functions.}}
      \item{Bayesian framework: \textcolor{red}{adherence to training data = likelihood}; \textcolor{blue}{regularisation = prior}.}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Density Estimation}
  \begin{block}{}
    \begin{itemize}
      \item{We want to do ML to do density estimation:}
      \item{Find the probability density $p$, given a set of training data $\{x_i\}$ that we suppose to be a fair random sample from $p$.}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{MAF}
  \begin{block}{}
    \begin{itemize}
      \item{We will discuss the MAF algorithm: Papamakarios et al. 2018; Masked Autoregressive Flow for Density Estimation; \url{https://arxiv.org/abs/1705.07057}.}
      \item{Based on earlier algorithms of which the most relevant is MADE: Germain et al. 2015; MADE: Masked Autoencoder for Distribution Estimation; \url{https://arxiv.org/abs/1502.03509}.}
	\item{I will focus first on MADE as all the key ideas are present there.}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Analogy}
  \begin{block}{}
    \begin{itemize}
      \item{An \textit{autoencoder} is like a parrot - it learns to mimic.}
	\item{An \textit{autoregressive autoencoder} is like a deaf parrot - it learns to play the probabilities (and hence becomes a density estimator).}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Parrot training}
  \begin{block}{}
    \begin{itemize}
      \item{Simple example: single binary outcome. We want the parrot to mimic us when we say 'Yes' or 'No'.}
	\item{So give lots of 'Yes', 'No' training data; give the parrot treats but reduce the treats by $\log(\text{\% correctness of the parrot's response})$ (a negative number). This is called \textit{cross-entropy loss}}
    \end{itemize}
  \end{block}
\end{frame}






\end{document}
